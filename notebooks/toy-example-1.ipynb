{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import elicito as el\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n"
   ],
   "id": "3ca96e6af85c69c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GenerativeModel:\n",
    "    def __call__(self, prior_samples, design_matrix, n_gr):\n",
    "        # extract prior samples per parameter type\n",
    "        betas=prior_samples[:,:,:-1]\n",
    "        sigma=prior_samples[:,:,-1][:,:,None]\n",
    "        # linear predictor\n",
    "        mu = tf.matmul(betas, design_matrix, transpose_b=True)\n",
    "        # data-generating model\n",
    "        y = tfd.Normal(loc=mu, scale=sigma).sample()\n",
    "        # selected observations per group\n",
    "        (y_gr0, y_gr1, y_gr2) = (y[:, :, i::] for i,j in zip(\n",
    "            [0,n_gr,2*n_gr],[n_gr,2*n_gr,-1]))\n",
    "        return dict(y_gr0=y_gr0, y_gr1=y_gr1,\n",
    "                    y_gr2=y_gr2, mu=mu, y=y)\n",
    "\n",
    "# design-matrix for three-level dummy-coded predictor\n",
    "def design_categorical(n_gr):\n",
    "    incpt = [1]*3\n",
    "    contrast_01 = [0, 1, 0]\n",
    "    contrast_02 = [0, 0, 1]\n",
    "    # contrast matrix\n",
    "    c = tf.stack([incpt, contrast_01, contrast_02], axis=-1)\n",
    "    # design matrix\n",
    "    x = tf.concat([tf.broadcast_to(c[i, :], (n_gr, c.shape[1])) for\n",
    "                   i in range(c.shape[0])], axis=0)\n",
    "    return tf.cast(x, tf.float32)\n",
    "\n",
    "model=el.model(\n",
    "    obj=GenerativeModel,\n",
    "    n_gr=30,\n",
    "    design_matrix=design_categorical(n_gr=30)\n",
    ")\n",
    "\n",
    "parameters=[\n",
    "    el.parameter(\n",
    "        name=f\"beta{i}\",\n",
    "        family=tfd.Normal,\n",
    "        hyperparams=dict(loc=el.hyper(f\"mu{i}\"),\n",
    "        scale=el.hyper(f\"sigma{i}\", lower=0))\n",
    "        ) for i in range(3)\n",
    "    ]+[\n",
    "    el.parameter(\n",
    "        name=\"sigma\",\n",
    "        family=tfd.HalfNormal,\n",
    "        hyperparams=dict(scale=el.hyper(\"sigma3\", lower=0))\n",
    "    ),\n",
    "]\n",
    "\n",
    "def r2(y, mu):\n",
    "    return tf.divide(\n",
    "        tf.math.reduce_variance(mu, axis=-1),\n",
    "        tf.math.reduce_variance(y, axis=-1)\n",
    "    )\n",
    "\n",
    "targets=[\n",
    "    el.target(\n",
    "        name=f\"y_gr{i}\",\n",
    "        query=el.queries.quantiles(quantiles=(.25, .50, .75)),\n",
    "        loss=el.losses.MMD2(kernel=\"energy\"),\n",
    "        ) for i in range(3)\n",
    "    ]+[\n",
    "    el.target(\n",
    "        name=\"r2\",\n",
    "        query=el.queries.quantiles(quantiles=(.25, .50, .75)),\n",
    "        loss=el.losses.MMD2(kernel=\"energy\"),\n",
    "        target_method=r2,\n",
    "        weight=10.\n",
    "    )\n",
    "]\n",
    "\n",
    "expert = el.expert.data(\n",
    "    dat={'quantiles_y_gr0': [0.64, 1.22, 1.89],\n",
    "        'quantiles_y_gr1': [0.72, 1.39, 2.07],\n",
    "        'quantiles_y_gr2': [0.76, 1.48, 2.22],\n",
    "        'quantiles_r2': [0.07, 0.23, 0.61]}\n",
    "    )\n",
    "\n",
    "optimizer=el.optimizer(\n",
    "    optimizer=tf.keras.optimizers.Adam,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "trainer=el.trainer(\n",
    "    method=\"parametric_prior\",\n",
    "    seed=1234,\n",
    "    # use reduced number of epoch to reduce run time\n",
    "    # epochs=600,\n",
    "    epochs=300,\n",
    "    progress=0\n",
    ")\n",
    "\n",
    "initializer=el.initializer(\n",
    "    method=\"sobol\",\n",
    "    iterations=32,\n",
    "    distribution=el.initialization.uniform(radius=2., mean=0.)\n",
    ")\n",
    "\n",
    "eliobj = el.Elicit(\n",
    "    model=model,\n",
    "    parameters=parameters,\n",
    "    targets=targets,\n",
    "    expert=expert,\n",
    "    optimizer=optimizer,\n",
    "    trainer=trainer,\n",
    "    initializer=initializer\n",
    ")"
   ],
   "id": "47f0f5a6b15af10a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eliobj.fit()\n",
    "# run only once when using notebooks via binder\n",
    "# due to computational cost\n",
    "# eliobj.fit(parallel=el.utils.parallel(runs=5))"
   ],
   "id": "4f5bc533c1cc848c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# eliobj.save(file=\"saved_eliobj/eliobj_param\")",
   "id": "214ee9cbb76b12b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "el.plots.loss(eliobj, figsize=(6,2))",
   "id": "1bf10104c9300d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "el.plots.hyperparameter(eliobj, figsize=(6,4))",
   "id": "332b00a27afdf21d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "el.plots.elicits(eliobj, figsize=(6,2))",
   "id": "a40c23e9f503f23c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "el.plots.prior_marginals(eliobj, figsize=(6,2))",
   "id": "78ae0eef57c68c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "el.plots.prior_averaging(eliobj, figsize=(6,3.5), xlim_weights=0.3)",
   "id": "8325ec0bc22ff8cb"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
